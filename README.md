# Analysis of political events using Twitter
- This is a project detail on how to apply Natual Language Processing techniques on tweets related to Ukraine war.
- All of below 9 tasks are done seperately in py files ti, tii, ..., tx in which ti, tii, ..., and tx stand for task 1,2, ..., 9. Using main.py file, you can access all of codes in the object folder. 
- You need to have these packages to be able to run the codes: pandas, nltk, numpy, matplotlib, spacy, gensim, ast, pickle, wordcloud, pyLDAvis, and empath.
- Input of main.py file is uploaded too. In the first task, the input is raw tweets .csv file. You can download it here (named march.csv). We retrieved initially about 340 k tweets from which only 240 k English tweets were extracted for further processing. However, please take into account that it is not possible to upload big file in GitHub, so march.csv file is a sample input to run the codes. In order to have access to more tweets for processing use: Ukraine Conflict Twitter Dataset (53.33M tweets) | Kaggle website. Please consider that you need to have a column of "text" in your input tweet csv file.
- In task 2, the raw tweets csv file are preprocess and cleaned. In this step, stopwords are excluded and those words that have meaning in english are kept using nltk package. This step takes time. To save time, you can use "clear_tok_neg.csv", "clear_tok_pos.csv", and "clear_tok_neut.csv" files in which negative, positve and neurtal tweets based on their sentiment scores (using nltk SentimentIntensityAnalyzer) are extracted. From task3 to 9, inputs are "clear_tok_neg.csv", "clear_tok_pos.csv", and "clear_tok_neut.csv" files.
- Final output of all tasks are uploaded in a file named "1. final outputs.7z"
- A presentation is uploaded to show background of the investigated topic, presenting results and providing more help on how codes are designed.

## Tasks description:
Consider The Twitter dataset of Ukraine conflict available at Ukraine Conflict Twitter Dataset (53.33M tweets) | Kaggle. As the dataset is high, so you may consider taking only a sample of dataset up to 100MB dataset size. You may look at examples in NLTK online book of script handling tweet dataset. 
1.	Suggest a script using a sentiment analyzer of your choice and outcomes the sentiment polarity of each tweet in the collection. Draw the histogram of positive, negative and neutral polarity tweets. The SentimentIntensityAnalyzer function of nltk was used to calculate “compound sentiment score” of each tweet. Out of 340 k tweets, the distribution of each English tweet (240 k) having neutral (sentiment score = 0), positive and negative sentiment scores is derived.
2.	We want group all posts of positive sentiment together as a single document, and those of negative sentiment together, and finally, neutral sentiment posts as a single document as well. For each of  these documents, perform LDA operation with number of topics =10 and number of words per topics =10 .
3.	Draw the WordCloud representation of each of the three documents, both preprocessing is carried out and without preprocessing phase.
4.	Use spacy to identify the named-entities in each of the above document and draw a histogram showing the 20 most frequent named-entities in each of the three documents.
5.	Consider the term “Russia”, and consider a window size of three on left and right hand side. Suggest a script that outputs the 20 most co-occurring words with Russia at window size 7 (three on left and three on right) in each of the three documents. Represent the outcome as a histogram.
6.	Run empath client https://github.com/Ejhfast/empath-client on each of the three documents and report categories which have no zero-value.  Discuss the potential overlapping between LDA results and empath client results.
7.	Now we would like to comprehend the public opinion in terms like, hate, trying to understand what user like / support and what they hate / dislike in each class. For this purpose, use wordnet to suggest a set of keywords semantically related to “hate” and a set of keywords semantically related to “like”. Write a script, which similarly to 5) identifies the list of word that co-occur with one of the keywords of hate (resp. like), at window size 7 and output the histogram of the 30 most co-occurring words for hate and 30 most co-occurring words for like, in each of the three documents. Suggest a manual categorization of these keywords in a way to ease explanation of the outcomes.
8.	We want to focus on modal verbs (e.g., shall, must, need) to comprehend what need/must/shall be performed in each class. For this purpose, for each class of document, record the sentences that contain modal verbs altogether. Perform a simple count analysis to identify the most frequent words in each class. Then perform named-entity tagger to output the most frequent named-entities in each case, then empath client to record the most dominant categories in each case. 
9.	Repeat the above process when considering “wish”. Identify a set of semantically equivalent wording to “wish” and repeat the process 8) to identify frequent words, frequent named-entities and categories.
